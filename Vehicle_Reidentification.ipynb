{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNi4PWPOfQuE7DTxCXkiq9e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shoreey/Vehicle-REID/blob/main/Vehicle_Reidentification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note: This code snippet is provided for educational and academic purposes.\n",
        "# The actual implementation was performed in a CLI conda environment and PyCharm.\n",
        "# Please refer to the documentation to get familiar with the environments and ensure you have the required libraries installed.\n"
      ],
      "metadata": {
        "id": "ZZVuCzNFeuK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Single Camera Vehicle tracking**"
      ],
      "metadata": {
        "id": "H1KTovhWc9tR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7cmLRTiygPZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPhWLaYcc6N5"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the YOLOv8 model\n",
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "# Open the video file\n",
        "video_path = \"E:\\\\Data Science\\\\Dissertation\\\\Shaurya\\\\AIC22_Track1_MTMC_Tracking\\\\DATASET\\\\c041.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Loop through the video frames\n",
        "while cap.isOpened():\n",
        "    # Read a frame from the video\n",
        "    success, frame = cap.read()\n",
        "\n",
        "    if success:\n",
        "        # Run YOLOv8 tracking on the frame, persisting tracks between frames\n",
        "        results = model.track(frame, persist=True)\n",
        "\n",
        "        # Visualize the results on the frame\n",
        "        annotated_frame = results[0].plot()\n",
        "\n",
        "        # Display the annotated frame\n",
        "        cv2.imshow(\"YOLOv8 Tracking\", annotated_frame)\n",
        "\n",
        "        # Break the loop if 'q' is pressed\n",
        "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "            break\n",
        "    else:\n",
        "        # Break the loop if the end of the video is reached\n",
        "        break\n",
        "\n",
        "# Release the video capture object and close the display window\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**image Cropping** **This code should be executed in the CLI**"
      ],
      "metadata": {
        "id": "f0liD2vQdQjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yolo task=detect mode=predict model=yolov8n.pt source=c041.mp4 tracker=botsort.yaml save_crop=True hide_labels=True hide_conf=True"
      ],
      "metadata": {
        "id": "MQSjVDmudULK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Feature Extractor #using example of HOG for getting started, RESNET-50 was used in the actual implementation"
      ],
      "metadata": {
        "id": "t_RrUqu_dwW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "MvBAFsciduTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Set paths to the directories containing cropped truck objects\n",
        "camera1_truck_dir = 'C:/Users/shaur/runs/detect/predict/crops/truck'\n",
        "camera2_truck_dir = 'C:/Users/shaur/runs/detect/predict2/crops/truck'\n",
        "\n",
        "# Initialize a feature extraction model (you can use a pretrained model)\n",
        "# For example, using OpenCV's HOG (Histogram of Oriented Gradients) for demonstration:\n",
        "hog = cv2.HOGDescriptor()\n",
        "\n",
        "# Initialize lists to store extracted features and their corresponding cameras\n",
        "truck_features = []\n",
        "cameras = []\n",
        "\n",
        "# Function to extract features from a list of images\n",
        "def extract_features_from_images(image_paths, feature_extractor):\n",
        "    features = []\n",
        "    for image_path in image_paths:\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is not None:\n",
        "            feature = feature_extractor.compute(image)\n",
        "            features.append(feature)\n",
        "    return features\n",
        "\n",
        "# Extract features for Camera 1 (c041)\n",
        "camera1_image_paths = [os.path.join(camera1_truck_dir, filename) for filename in os.listdir(camera1_truck_dir)]\n",
        "camera1_features = extract_features_from_images(camera1_image_paths, hog)\n",
        "truck_features.extend(camera1_features)\n",
        "cameras.extend(['c041'] * len(camera1_features))\n",
        "\n",
        "# Extract features for Camera 2 (c042)\n",
        "camera2_image_paths = [os.path.join(camera2_truck_dir, filename) for filename in os.listdir(camera2_truck_dir)]\n",
        "camera2_features = extract_features_from_images(camera2_image_paths, hog)\n",
        "truck_features.extend(camera2_features)\n",
        "cameras.extend(['c042'] * len(camera2_features))\n",
        "\n",
        "# Convert features and camera labels to NumPy arrays\n",
        "truck_features = np.array(truck_features)\n",
        "cameras = np.array(cameras)\n",
        "\n",
        "# Create a Pandas DataFrame to store the features and camera labels\n",
        "data = {'Feature': list(truck_features), 'Camera': cameras}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('truck_features.csv', index=False)"
      ],
      "metadata": {
        "id": "JXOdzoDddksr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query System #feedback Mechanism**"
      ],
      "metadata": {
        "id": "AMIskNIId8ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load the Excel file containing truck filenames\n",
        "matched_trucks_df = pd.read_excel(\"C:/Users/shaur/PycharmProjects/pythonProject3/matching_trucks.xlsx\")\n",
        "\n",
        "# Directory paths for c041 and c042\n",
        "c041_dir = 'C:/Users/shaur/runs/detect/predict/crops/truck'\n",
        "c042_dir = 'C:/Users/shaur/runs/detect/predict2/crops/truck'\n",
        "\n",
        "# Function to display matched trucks and gather user feedback\n",
        "def display_matched_trucks(truck1_filename, truck2_filename):\n",
        "    truck1_image_path = os.path.join(c041_dir, truck1_filename)\n",
        "    truck2_image_path = os.path.join(c042_dir, truck2_filename)\n",
        "\n",
        "    # Load images with error handling\n",
        "    truck1_image = cv2.imread(truck1_image_path)\n",
        "    truck2_image = cv2.imread(truck2_image_path)\n",
        "\n",
        "    if truck1_image is None or truck2_image is None:\n",
        "        print(f\"Error loading one or both images: {truck1_image_path}, {truck2_image_path}\")\n",
        "        return\n",
        "\n",
        "    # Display images\n",
        "    cv2.imshow('Matched Truck from Camera 1', truck1_image)\n",
        "    cv2.imshow('Matched Truck from Camera 2', truck2_image)\n",
        "\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Function to display matched trucks and gather user feedback\n",
        "def display_and_confirm_matches(truck1_filename, truck2_filename):\n",
        "    display_matched_trucks(truck1_filename, truck2_filename)\n",
        "\n",
        "    # Gather user feedback\n",
        "    feedback = input(\"Are these matches correct? (y/n): \")\n",
        "\n",
        "    return feedback.lower() == 'y'\n",
        "\n",
        "# Create a list to store user feedback and matching column\n",
        "user_feedback = []\n",
        "matching_column = []  # New column to store 'Yes' or 'No'\n",
        "\n",
        "# Iterate through the matched truck pairs and gather user feedback\n",
        "for idx, row in matched_trucks_df.iterrows():\n",
        "    truck1_filename = row['Truck1']\n",
        "    truck2_filename = row['Truck2']\n",
        "\n",
        "    print(f\"Pair: {truck1_filename}, {truck2_filename}\")\n",
        "\n",
        "    if os.path.exists(os.path.join(c041_dir, truck1_filename)) and os.path.exists(\n",
        "            os.path.join(c042_dir, truck2_filename)):\n",
        "        if display_and_confirm_matches(truck1_filename, truck2_filename):\n",
        "            user_feedback.append('Yes')\n",
        "            matching_column.append('Yes')\n",
        "        else:\n",
        "            user_feedback.append('No')\n",
        "            matching_column.append('No')\n",
        "    else:\n",
        "        print(f\"Skipping missing images: {truck1_filename}, {truck2_filename}\")\n",
        "        user_feedback.append('N/A')\n",
        "        matching_column.append('N/A')\n",
        "\n",
        "# Add user feedback and matching column to the DataFrame\n",
        "matched_trucks_df['UserFeedback'] = user_feedback\n",
        "matched_trucks_df['Matching'] = matching_column\n",
        "\n",
        "# Save the results to an Excel file\n",
        "matched_trucks_df.to_excel('matched_trucks_with_feedback.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "ixPdfwO5d8Fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assigning Id's # Vehicle Reidentification**"
      ],
      "metadata": {
        "id": "awCANCEQeKtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import cv2\n",
        "import tkinter as tk\n",
        "import numpy as np  # Add this import\n",
        "\n",
        "# Load the Excel file containing matched trucks with feedback and assigned IDs\n",
        "matched_trucks_df = pd.read_excel('C:/Users/shaur/PycharmProjects/pythonProject3/matched_trucks_with_feedback.xlsx')\n",
        "\n",
        "# Directory paths for c041 and c042\n",
        "c041_dir = 'C:/Users/shaur/runs/detect/predict/crops/truck'\n",
        "c042_dir = 'C:/Users/shaur/runs/detect/predict2/crops/truck'\n",
        "\n",
        "# Function to display matched trucks and their IDs\n",
        "def display_matched_trucks_with_ids(truck1_filename, truck2_filename, assigned_id):\n",
        "    truck1_image_path = os.path.join(c041_dir, truck1_filename)\n",
        "    truck2_image_path = os.path.join(c042_dir, truck2_filename)\n",
        "\n",
        "    # Load images with error handling\n",
        "    truck1_image = cv2.imread(truck1_image_path)\n",
        "    truck2_image = cv2.imread(truck2_image_path)\n",
        "\n",
        "    if truck1_image is None or truck2_image is None:\n",
        "        print(f\"Error loading one or both images: {truck1_image_path}, {truck2_image_path}\")\n",
        "        return\n",
        "\n",
        "    # Display images and IDs for non-NA values\n",
        "    if not pd.isna(assigned_id):\n",
        "        cv2.putText(truck1_image, f\"ID: {assigned_id}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "        cv2.putText(truck2_image, f\"ID: {assigned_id}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "\n",
        "        # Show images with IDs\n",
        "        cv2.imshow('Matched Truck from Camera 1', truck1_image)\n",
        "        cv2.imshow('Matched Truck from Camera 2', truck2_image)\n",
        "\n",
        "        cv2.waitKey(0)\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "# Create a Tkinter window\n",
        "root = tk.Tk()\n",
        "root.withdraw()  # Hide the main window\n",
        "\n",
        "# Iterate through the rows of the DataFrame\n",
        "for idx, row in matched_trucks_df.iterrows():\n",
        "    truck1_filename = row['Truck1']\n",
        "    truck2_filename = row['Truck2']\n",
        "    assigned_id = row['AssignedID']\n",
        "\n",
        "    # Display matched trucks with their assigned IDs (skip rows with 'NA' in AssignedID)\n",
        "    if not pd.isna(assigned_id):\n",
        "        display_matched_trucks_with_ids(truck1_filename, truck2_filename, assigned_id)\n",
        "\n",
        "# Close the Tkinter window\n",
        "root.destroy()\n"
      ],
      "metadata": {
        "id": "RFx43DqvePJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Traffic Tracking and Visualization**"
      ],
      "metadata": {
        "id": "jKfzHmTQeZlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from itertools import cycle\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load the YOLOv8 model\n",
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "# Open the video file\n",
        "video_path = \"path/to/video.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Store the track history\n",
        "track_history = defaultdict(lambda: [])\n",
        "\n",
        "# Lists to store the final positions for plotting\n",
        "all_x_positions = []\n",
        "all_y_positions = []\n",
        "\n",
        "# Define a cycle of colors for tracks\n",
        "track_colors = cycle(['b', 'g', 'r', 'c', 'm', 'y', 'k'])\n",
        "\n",
        "# Maximum number of tracks to display\n",
        "max_tracks = 9\n",
        "\n",
        "# Maximum number of labels to display\n",
        "max_labels = 7\n",
        "\n",
        "# Lists to store the legend labels and colors\n",
        "legend_labels = []\n",
        "legend_colors = []\n",
        "\n",
        "# Loop through the video frames\n",
        "while cap.isOpened():\n",
        "    # Read a frame from the video\n",
        "    success, frame = cap.read()\n",
        "\n",
        "    if success:\n",
        "        # Run YOLOv8 tracking on the frame, persisting tracks between frames\n",
        "        results = model.track(frame, persist=True)\n",
        "\n",
        "        # Get the boxes and track IDs\n",
        "        boxes = results[0].boxes.xywh.cpu()\n",
        "        track_ids = results[0].boxes.id.int().cpu().tolist()\n",
        "\n",
        "        # Plot the tracks and labels\n",
        "        plotted_labels = set()  # To keep track of plotted labels\n",
        "        for box, track_id in zip(boxes, track_ids):\n",
        "            x, y, w, h = box\n",
        "            track = track_history[track_id]\n",
        "            track.append((float(x), float(y)))  # x, y center point\n",
        "            if len(track) > max_tracks:  # limit the number of points in the track history\n",
        "                track.pop(0)\n",
        "\n",
        "            # Append the positions to the lists\n",
        "            all_x_positions.extend([point[0] for point in track])\n",
        "            all_y_positions.extend([point[1] for point in track])\n",
        "\n",
        "            # Get the next unique color for the current track\n",
        "            color = next(track_colors)\n",
        "\n",
        "            # Plot the track with label (up to max_labels)\n",
        "            if len(plotted_labels) < max_labels:\n",
        "                label = f\"Track {track_id}\"\n",
        "                plt.plot([point[0] for point in track], [point[1] for point in track], label=label, color=color)\n",
        "                legend_labels.append(label)\n",
        "                legend_colors.append(color)\n",
        "                plotted_labels.add(label)\n",
        "\n",
        "    else:\n",
        "        # Break the loop if the end of the video is reached\n",
        "        break\n",
        "\n",
        "# Release the video capture object\n",
        "cap.release()\n",
        "\n",
        "# Create the final plot without a legend\n",
        "plt.xlabel('X Position')\n",
        "plt.ylabel('Y Position')\n",
        "plt.title('Traffic Movement')\n",
        "plt.grid(True)\n",
        "\n",
        "# Move the legend outside of the graph\n",
        "plt.legend(handles=[plt.Line2D([0], [0], color=color, label=label) for color, label in zip(legend_colors[:max_labels], legend_labels[:max_labels])],\n",
        "           loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.subplots_adjust(right=0.7)  # Adjust the space for the legend\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oTpw2LJoelVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MH48T9doestd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}